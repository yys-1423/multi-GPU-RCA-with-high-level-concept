# Core ML Libraries
torch>=2.1.0
transformers>=4.36.0
accelerate>=0.25.0
datasets>=2.15.0

# DeepSpeed for distributed training
deepspeed>=0.12.0

# LoRA for parameter-efficient fine-tuning
peft>=0.7.0
bitsandbytes>=0.41.0

# Flash Attention (requires separate installation)
# pip install flash-attn --no-build-isolation
# flash-attn>=2.3.0

# Monitoring and logging
wandb>=0.16.0
tensorboard>=2.15.0

# Utilities
numpy>=1.24.0
tqdm>=4.66.0
sentencepiece>=0.1.99
protobuf>=4.25.0

# Optional: For tokenization speed
tokenizers>=0.15.0
